{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.base import clone\n",
    "import timeit\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca(F, X):\n",
    "    n, d = X.shape\n",
    "    mu = np.zeros((d, 1))\n",
    "    Z = np.zeros((d, F))\n",
    "    for i in range(d):\n",
    "        mu[i] = (1. / n) * np.sum(X[:, [i]])\n",
    "    X = X - mu.T\n",
    "    U, s, Vt = la.svd(X, False)\n",
    "    g = s[:F]\n",
    "    for i in range(F):\n",
    "        g[i] = 1. / g[i]\n",
    "    W = Vt[:F]\n",
    "    Z = np.dot(W.T, np.diag(g))\n",
    "    return (mu, Z)\n",
    "\n",
    "def pca_proj(X,mu,Z):\n",
    "    n, d = X.shape\n",
    "    X = X - mu.T\n",
    "    return np.dot(X, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold(k, model, f, X, y, error_type=\"mse\"):\n",
    "    n, d = X.shape\n",
    "    z = np.zeros((k, 1))\n",
    "    for i in range(k):\n",
    "        T = list(range(int((i * n) / k), int((n * (i + 1) / k))))\n",
    "        S = [j for j in range(n) if j not in T]\n",
    "        curr_model = clone(model)\n",
    "\n",
    "        training_mu, training_Z = pca(f, X[S])\n",
    "        training_X = pca_proj(X[S], training_mu, training_Z)\n",
    "\n",
    "        curr_model.fit(training_X, y[S])\n",
    "\n",
    "        test_X = pca_proj(X[T], training_mu, training_Z)\n",
    "\n",
    "        # y[T] will be len(T) by 1\n",
    "        # X[T] will be len(T) by d\n",
    "        if error_type == \"mse\":\n",
    "            z[i] = (1. / len(T)) * np.sum((y[T] - curr_model.predict(test_X)) ** 2)\n",
    "        elif error_type == \"log_mse\":\n",
    "            z[i] = (1. / len(T)) * np.sum((np.log(y[T] + 1) - np.log(curr_model.predict(test_X) + 1)) ** 2)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bootstrapping(B, model, f, X, y, error_type=\"mse\"):\n",
    "    n, d = X.shape\n",
    "    z = np.zeros((B, 1))\n",
    "    for i in range(B):\n",
    "        u = np.random.choice(n, n, replace=True)\n",
    "        S = np.unique(u)\n",
    "        T = np.setdiff1d(np.arange(n), S, assume_unique=True)\n",
    "        curr_model = clone(model)\n",
    "\n",
    "        training_mu, training_Z = pca(f, X[u])\n",
    "        training_X = pca_proj(X[u], training_mu, training_Z)\n",
    "\n",
    "        curr_model.fit(training_X, y[u])\n",
    "\n",
    "        test_X = pca_proj(X[T], training_mu, training_Z)\n",
    "\n",
    "        # y[T] will be len(T) by 1\n",
    "        # X[T] will be len(T) by d\n",
    "        # theta_hat will be d by 1\n",
    "        if error_type == \"mse\":\n",
    "            z[i] = (1. / len(T)) * np.sum((y[T] - curr_model.predict(test_X)) ** 2)\n",
    "        elif error_type == \"log_mse\":\n",
    "            z[i] = (1. / len(T)) * np.sum((np.log(y[T] + 1) - np.log(curr_model.predict(test_X) + 1)) ** 2)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, f, X, y, k=5, B=5, error = \"mse\"):\n",
    "    ########################KFOLD###################\n",
    "    print('Evaluating K-fold with %d folds.' % k)\n",
    "    start_time = timeit.default_timer()\n",
    "    k_fold_z = k_fold(k, model, f, X, y, error_type=error)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    \n",
    "    k_fold_mse = np.mean(k_fold_z)\n",
    "    print('K-fold %s: ' % error, k_fold_mse)\n",
    "    \n",
    "    k_fold_rmse = math.sqrt(k_fold_mse)\n",
    "    print('K-fold Square Root %s: ' % error, k_fold_rmse)\n",
    "\n",
    "    print(\"Time elapsed for k-fold: \", elapsed)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    ###################BOOTSTRAPPING################\n",
    "    print('Evaluating bootstrapping with %d bootstraps.' % B)\n",
    "    start_time = timeit.default_timer()\n",
    "    bootstrapping_z = bootstrapping(B, model, f, X, y)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    \n",
    "    bootstrapping_mse = np.mean(bootstrapping_z)\n",
    "    print('Bootstrapping Mean Squared Error: ', bootstrapping_mse)\n",
    "    \n",
    "    bootstrapping_rmse = math.sqrt(bootstrapping_mse)\n",
    "    print('Bootstrapping Square Root Mean Squared Error: ', bootstrapping_rmse)\n",
    "\n",
    "    print(\"Time elapsed for bootstrapping: \", elapsed)\n",
    "    \n",
    "    return (k_fold_z, k_fold_mse, k_fold_rmse, bootstrapping_z, bootstrapping_mse, bootstrapping_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 80)\n",
      "(1460, 1)\n",
      "(1459, 80)\n"
     ]
    }
   ],
   "source": [
    "kaggle_train = pd.read_csv(\"train.csv\", header=0)\n",
    "kaggle_test = pd.read_csv(\"test.csv\", header=0)\n",
    "\n",
    "kaggle_train_X = kaggle_train.iloc[:,:-1]\n",
    "kaggle_train_y = kaggle_train.iloc[:,-1:]\n",
    "\n",
    "print(kaggle_train_X.shape)\n",
    "print(kaggle_train_y.shape)\n",
    "print(kaggle_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins Kaggle Train and Test data together so we can encode at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_full = kaggle_train_X.append(kaggle_test, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drops columns that have more than 40% null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_drop = []\n",
    "data_keys = X_full.keys()\n",
    "for i, b in enumerate((X_full.isnull().sum() / X_full.shape[0]) > 0.4):\n",
    "    if b:\n",
    "        cols_to_drop.append(data_keys[i])\n",
    "\n",
    "X_full_dropped = X_full.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs one hot encoding for categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catCols = set(X_full_dropped.select_dtypes(include=['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "for col in catCols:\n",
    "    oneHot_encoded = pd.get_dummies(X_full_dropped[col])\n",
    "    oneHot_encoded = oneHot_encoded.add_prefix(col + '_is_')\n",
    "    frames.append(oneHot_encoded)\n",
    "\n",
    "X_full_ohe = X_full_dropped.drop(catCols, axis=1)\n",
    "X_full_ohe = pd.concat([X_full_ohe, pd.concat(frames, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splits data back into Kaggle Train and Kaggle Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_train_X_psplit = X_full_ohe.loc[X_full_ohe[\"Id\"].between(1,1460)]\n",
    "kaggle_test_X_psplit = X_full_ohe.loc[X_full_ohe[\"Id\"].between(1461, 1460+1459)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks for null values and counts for both rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "11\n",
      "339\n",
      "313\n"
     ]
    }
   ],
   "source": [
    "# cols\n",
    "print(kaggle_train_X_psplit.loc[:, kaggle_train_X_psplit.isnull().any()].shape[1])\n",
    "print(kaggle_test_X_psplit.loc[:, kaggle_test_X_psplit.isnull().any()].shape[1])\n",
    "\n",
    "# rows\n",
    "print(kaggle_train_X_psplit.loc[kaggle_train_X_psplit.isnull().any(axis=1)].shape[0])\n",
    "print(kaggle_test_X_psplit.loc[kaggle_test_X_psplit.isnull().any(axis=1)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replaces null values with medians of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_train_X_processed = kaggle_train_X_psplit.fillna(kaggle_train_X_psplit.median())\n",
    "kaggle_test_X_processed = kaggle_test_X_psplit.fillna(kaggle_test_X_psplit.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks for null values and counts for both rows/columns once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# cols\n",
    "print(kaggle_train_X_processed.loc[:, kaggle_train_X_processed.isnull().any()].shape[1])\n",
    "print(kaggle_test_X_processed.loc[:, kaggle_test_X_processed.isnull().any()].shape[1])\n",
    "\n",
    "# rows\n",
    "print(kaggle_train_X_processed.loc[kaggle_train_X_processed.isnull().any(axis=1)].shape[0])\n",
    "print(kaggle_test_X_processed.loc[kaggle_test_X_processed.isnull().any(axis=1)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Kaggle Training into Train/Test since Kaggle Test has no response variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 80:20 train test ratio\n",
    "test_size = 0.2\n",
    "# This function splits the training and target sets into random train and test subsets.\n",
    "# X_train and X_test are subsets of the training data\n",
    "# y_train and y_test are subsets the the target data\n",
    "X_train, X_test, y_train, y_test = train_test_split(kaggle_train_X_processed, kaggle_train_y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selects 50 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = 50\n",
    "f = F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Runs PCA for 50 features on Kaggle train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_kaggle_train_mu, X_kaggle_train_Z = pca(F, kaggle_train_X_processed.values)\n",
    "X_kaggle_train_pca = pca_proj(kaggle_train_X_processed.values, X_kaggle_train_mu, X_kaggle_train_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Projects PCA for Kaggle Test onto Kaggle Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_kaggle_test_pca = pca_proj(kaggle_test_X_processed.values, X_kaggle_train_mu, X_kaggle_train_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Runs PCA for 50 features on the train split of Kaggle train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_mu, X_train_Z = pca(F, X_train.values)\n",
    "X_train_pca = pca_proj(X_train.values, X_train_mu, X_train_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Projects PCA for test split onto train split of Kaggle train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_pca = pca_proj(X_test.values, X_train_mu, X_train_Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "adaBoost = AdaBoostRegressor(n_estimators=70)\n",
    "ada_k_z, ada_k_mse, ada_k_rmse, ada_b_z, ada_b_mse, ada_b_rmse = evaluate_model(adaBoost, f, kaggle_train_X_processed.values, kaggle_train_y.values.ravel(), k=5, B=5)\n",
    "\n",
    "adaBoost.fit(X_train_pca, y_train.values.ravel())\n",
    "adaBoost.score(X_test_pca, y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Predicted values\n",
    "predicted = adaBoost.predict(X_test_pca)\n",
    "ada_pred = y_test.copy()\n",
    "ada_pred['predicted'] = predicted\n",
    "ada_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor(max_depth=3, learning_rate=0.2, booster='gbtree', n_estimators=70)\n",
    "xg_k_z, xg_k_mse, xg_k_rmse, xg_b_z, xg_b_mse, xg_b_rmse = evaluate_model(xgb, f, kaggle_train_X_processed.values, kaggle_train_y.values.ravel(), k=5, B=5)\n",
    "\n",
    "xgb.fit(X_train_pca, y_train)\n",
    "xgb.score(X_test_pca, y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = xgb.predict(X_test_pca)\n",
    "xgb_pred = y_test.copy()\n",
    "xgb_pred['predicted'] = predicted\n",
    "xgb_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svr_model = svm.SVR(kernel=\"poly\", coef0=-2500, gamma=\"auto\")\n",
    "# coef0 only works with poly and sigmoid kernels\n",
    "# it just puts that value instead of the column of 1's\n",
    "\n",
    "# without it, this model breaks for some reason\n",
    "\n",
    "svr_k_z, svr_k_mse, svr_k_rmse, svr_b_z, svr_b_mse, svr_b_rmse = evaluate_model(svr_model, f, kaggle_train_X_processed.values, kaggle_train_y.values.ravel(), k=5, B=5)\n",
    "\n",
    "# epsilon, degree\n",
    "svr_model.fit(X_train_pca, y_train.values.ravel())\n",
    "svr_model.score(X_test_pca, y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svr_predicted = svr_model.predict(X_test_pca)\n",
    "svr_pred = y_test.copy()\n",
    "svr_pred[\"predicted\"] = svr_predicted\n",
    "svr_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plots of predictions for all 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotScatter(predicted, name):\n",
    "    colors = [\"r\", \"b\"]\n",
    "    plt.title(name + \" Predicted vs Actual Sale Price\")\n",
    "    plt.xlabel(\"Actual Sale Price\")\n",
    "    plt.ylabel(\"Predicted Sale Price\")\n",
    "    red_patch = mpatches.Patch(color='red', label='Actual Sale Price')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Predicted Sale Price')\n",
    "    plt.scatter(predicted['SalePrice'], predicted['predicted'], color=\"r\", alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"========== Prediction Scatter Plots Across Models ==========\")\n",
    "print()\n",
    "\n",
    "plotScatter(ada_pred, \"AdaBoost\")\n",
    "plotScatter(xgb_pred, \"XgBoost\")\n",
    "plotScatter(svr_pred, \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_xgb = XGBRegressor(max_depth=3, learning_rate=0.2, booster='gbtree', n_estimators=70)\n",
    "\n",
    "kaggle_xgb.fit(X_kaggle_train_pca, kaggle_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_predicted = kaggle_xgb.predict(X_kaggle_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_predicted_complete = pd.DataFrame({'Id': kaggle_test[\"Id\"], 'SalePrice': kaggle_predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_predicted_complete.to_csv('kaggle_predicted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunes AdaBoost/XGBoost n_estimators and SVR's C parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This takes time (~2mins)\n",
    "def tune_hyperparameters(X, Y, f):\n",
    "    adaboost_param_tuning = pd.DataFrame(columns=['parameter', 'rmse'])\n",
    "    xgb_param_tuning = pd.DataFrame(columns=['parameter', 'rmse'])    \n",
    "    svr_param_tuning = pd.DataFrame(columns=['parameter', 'rmse'])    \n",
    "\n",
    "    #Tuning n estimators parameter for boosting algorithms\n",
    "    for i in range(25,200,25):\n",
    "        print(\"Boosting: \" + str(i))\n",
    "        print()\n",
    "        adaBoost = AdaBoostRegressor(n_estimators=i)\n",
    "        print(adaBoost)\n",
    "        k_fold_z, k_fold_mse, k_fold_rmse, bootstrapping_z, bootstrapping_mse, bootstrapping_rmse = evaluate_model(adaBoost, f, X, Y.values.ravel(), k=5, B=5)\n",
    "        adaboost_param_tuning = adaboost_param_tuning.append({'parameter': i, 'rmse': k_fold_rmse}, ignore_index=True)\n",
    "        print()\n",
    "        \n",
    "        xgb = XGBRegressor(max_depth=3, learning_rate=0.2, booster='gbtree', n_estimators=i)\n",
    "        print(xgb)\n",
    "        k_fold_z, k_fold_mse, k_fold_rmse, bootstrapping_z, bootstrapping_mse, bootstrapping_rmse = evaluate_model(xgb, f, X, Y.values.ravel(), k=5, B=5)\n",
    "        xgb_param_tuning = xgb_param_tuning.append({'parameter': i, 'rmse': k_fold_rmse}, ignore_index=True)\n",
    "        print()\n",
    "        \n",
    "    #for i in range(25,200,25):\n",
    "    c_vals = [0.01, 0.1, 10, 100]\n",
    "    for i in c_vals:\n",
    "        print(\"C: \" + str(i))\n",
    "        print()\n",
    "        svr_model = svm.SVR(kernel=\"poly\", coef0=-2500, gamma=\"auto\", C=i)\n",
    "        print(svr_model)\n",
    "        k_fold_z, k_fold_mse, k_fold_rmse, bootstrapping_z, bootstrapping_mse, bootstrapping_rmse = evaluate_model(svr_model, f, X, Y.values.ravel(), k=5, B=5)\n",
    "        svr_param_tuning = svr_param_tuning.append({'parameter': i, 'rmse': k_fold_rmse}, ignore_index=True)\n",
    "        print()\n",
    "\n",
    "    return xgb_param_tuning, adaboost_param_tuning, svr_param_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"========== Hyper-Parameter Tuning ==========\")\n",
    "print()\n",
    "\n",
    "xgb_params, adaboost_params, svm_params = tune_hyperparameters(kaggle_train_X_processed.values, kaggle_train_y, f)\n",
    "\n",
    "print()\n",
    "print(\"*** Hyper-parameter Tuning Complete! ***\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots how the changing of hyperparameters changes our models' root mean squared log errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_param_tuning(xgb_params, adaboost_params, svm_params):\n",
    "    plt.plot(adaboost_params['parameter'], adaboost_params['rmse'], marker='o', color='b')\n",
    "    plt.title(\"Adaboost RMSE vs n_estimators\")\n",
    "    plt.xlabel(\"n_estimators\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(xgb_params['parameter'], xgb_params['rmse'], marker='o', color='b')\n",
    "    plt.title(\"XgBoost RMSE vs n_estimators\")\n",
    "    plt.xlabel(\"n_estimators\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(svm_params['parameter'], svm_params['rmse'], marker='o', color='b')\n",
    "    plt.title(\"SVM RMSE vs C\")\n",
    "    plt.xlabel(\"C\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"========== Parameter Tuning Plots ==========\")\n",
    "print()\n",
    "\n",
    "plot_param_tuning(xgb_params, adaboost_params, svm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunes how many features to use from PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This takes time to run (~5mins)\n",
    "def pca_tuning(F, X, Y):\n",
    "    \n",
    "    pca_ada_frame = pd.DataFrame(columns=['pca', 'rmse'])\n",
    "    pca_xg_frame = pd.DataFrame(columns=['pca', 'rmse'])\n",
    "    pca_svr_frame = pd.DataFrame(columns=['pca', 'rmse'])\n",
    "    \n",
    "    for f in F:\n",
    "        print(\"\\nPCA %d\" % f)\n",
    "\n",
    "        # AdaBoost\n",
    "        print(\"\\n\")\n",
    "        from sklearn.ensemble import AdaBoostRegressor\n",
    "        adaBoost = AdaBoostRegressor()\n",
    "        k_z, k_mse, k_rmse, b_z, b_mse, b_rmse = evaluate_model(adaBoost, f, X.values, Y.values.ravel(), k=5, B=5)\n",
    "        pca_ada_frame = pca_ada_frame.append({'pca': f, 'rmse': k_rmse}, ignore_index=True)\n",
    "\n",
    "\n",
    "        # XGBoost Regressor\n",
    "        print(\"\\nXGBoost\")\n",
    "        from xgboost import XGBRegressor\n",
    "\n",
    "        xgb = XGBRegressor(max_depth=3, learning_rate=0.2, booster='gbtree', n_estimators=70)\n",
    "\n",
    "        k_z, k_mse, k_rmse, b_z, b_mse, b_rmse = evaluate_model(xgb, f, X.values, Y.values.ravel(), k=5, B=5)\n",
    "        pca_xg_frame = pca_xg_frame.append({'pca': f, 'rmse': k_rmse}, ignore_index=True)\n",
    "\n",
    "        \n",
    "        # SVM (SVR)\n",
    "        print(\"\\nSVR\")\n",
    "        from sklearn import svm\n",
    "\n",
    "        svr_model = svm.SVR(kernel=\"poly\", coef0=-3500, gamma='auto')\n",
    "        # coef0 only works with poly and sigmoid kernels\n",
    "        # it just puts that value instead of the column of 1's\n",
    "\n",
    "        # without it, this model breaks for some reason\n",
    "\n",
    "        k_z, k_mse, k_rmse, b_z, b_mse, b_rmse = evaluate_model(svr_model, f, X.values, Y.values.ravel(), k=5, B=5)\n",
    "        pca_svr_frame = pca_svr_frame.append({'pca': f, 'rmse': k_rmse}, ignore_index=True)\n",
    "        \n",
    "    return (pca_ada_frame, pca_xg_frame, pca_svr_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots how different amounts of PCA features affects our models' performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pca_tuning(ada_data, xg_data, svr_data):\n",
    "    plt.title(\"RMSE vs PCA Values across models\")\n",
    "    plt.xlabel(\"PCA Values\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.grid()\n",
    "    plt.plot(ada_data['pca'], ada_data['rmse'], marker='o', color='r', label=\"Adaboost\")\n",
    "    plt.plot(xg_data['pca'], xg_data['rmse'], marker='o', color='g', label=\"XgBoost\")\n",
    "    plt.plot(svr_data['pca'], svr_data['rmse'], marker='o', color='b', label=\"SVR\")\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"========== PCA Tuning ==========\")\n",
    "print()\n",
    "\n",
    "pca_ada_frame, pca_xg_frame, pca_svr_frame = pca_tuning([30,35,40,45,50,55,60,65,70], kaggle_train_X_processed, kaggle_train_y)\n",
    "\n",
    "print()\n",
    "print(\"*** PCA Tuning Complete ***\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_tuning(pca_ada_frame, pca_xg_frame, pca_svr_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"========== Learning Curves ==========\")\n",
    "print()\n",
    "plot_learning_curve(estimator=adaBoost, title=\"Learning Curves (AdaBoost)\", X=kaggle_train_X_processed, y=kaggle_train_y.values.ravel(), ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "plot_learning_curve(estimator=xgb, title=\"Learning Curves (XgBoost)\", X=kaggle_train_X_processed, y=kaggle_train_y.values.ravel(), ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "plot_learning_curve(estimator=svr_model, title=\"Learning Curves (SVR)\", X=kaggle_train_X_processed, y=kaggle_train_y.values.ravel(), ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
